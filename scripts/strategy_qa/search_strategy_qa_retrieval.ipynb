{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from tot.prompts.strategy_qa import propose_prompt, value_prompt, answer_prompt_retrieval\n",
    "from tot.models import gpt, gpt_usage\n",
    "from tot.tasks.strategy_qa import StrategyQAEnv\n",
    "from tot.methods.evaluation import compute_f1_strategy\n",
    "import os\n",
    "\n",
    "env = StrategyQAEnv()\n",
    "api_key = os.getenv('OPENAI_API_KEY', 'PUT-YOUR-KEY-HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"facts.txt\")\n",
    "doc = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator = '\\n')\n",
    "splits = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "persist_directory = 'docs/chroma/'\n",
    "embedding = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "# print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=api_key)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_tokens = prompt_tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_prompt_wrap(obs):\n",
    "    global completion_tokens, prompt_tokens\n",
    "    prompt = propose_prompt.format(input=obs)\n",
    "    prompt_tokens += len(prompt.split())\n",
    "    # print(prompt)\n",
    "    return prompt\n",
    "\n",
    "# print(propose_prompt_wrap(env.reset(0)))\n",
    "# print('---------')\n",
    "# print(prompt_wrap(env.step('h2. value')[0]))\n",
    "\n",
    "# def answer_prompt_wrap(self):\n",
    "#     global completion_tokens, prompt_tokens\n",
    "#     prompt = answer_prompt.format(original_question=self.data, facts=self.facts_gt, decomposition=self.decomp)\n",
    "#     prompt_tokens += len(prompt.split())\n",
    "#     # print(prompt)\n",
    "#     return prompt\n",
    "\n",
    "def answer_prompt_wrap(final_decomp, original_question):\n",
    "    global completion_tokens, prompt_tokens\n",
    "    prompt = answer_prompt_retrieval.format(original_question=original_question, decomposition=final_decomp)\n",
    "    prompt_tokens += len(prompt.split())\n",
    "    # print(prompt)\n",
    "    return prompt\n",
    "\n",
    "# print(answer_prompt_wrap(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "from tot.models import gpt\n",
    "\n",
    "def parse_line(input_str):\n",
    "    pattern = r'confidence level: (\\w+)'\n",
    "    match = re.search(pattern, input_str, re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        confidence_level = match.group(1)\n",
    "        return confidence_level\n",
    "    else:\n",
    "        print(\"No confidence level found.\")\n",
    "\n",
    "confidence_to_value = {'stop': 100, 'certain': 1, 'high': 0.5, 'medium': 0.2, 'low': 0.1}  # TODO: ad hoc\n",
    "\n",
    "def parse_response(response):\n",
    "    response = response.split('\\n')\n",
    "    score = parse_line(response[-1].strip())\n",
    "    candidate = response[:-1]\n",
    "    candidate = '\\n'.join(candidate)\n",
    "\n",
    "    parsed_lines = [(candidate, confidence_to_value.get(score, 0))]\n",
    "    return parsed_lines\n",
    "\n",
    "\n",
    "def get_candidates_to_scores(env):\n",
    "    global completion_tokens, prompt_tokens\n",
    "    obs = env.render()\n",
    "    if obs in env.cache: # Checks if the observation (obs) is already present in the cache of the environment (env)\n",
    "        print('cache hit: stopping the decomposition because of repetition!')\n",
    "        return []\n",
    "        # return env.cache[obs] # If so returns the cached result (candidates and their scores)\n",
    "\n",
    "    print('call gpt')\n",
    "    responses = gpt(propose_prompt_wrap(obs), n=3) # get candidates for possible next steps\n",
    "    # responses = ['Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia solve altogether in April and May?\\n    - How many clips did Natalia sell in April?\\n    - How many clips did Natalia sell in June?\\nConfidence level: high', 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia solve altogether in April and May?\\n    - How many clips did Natalia sell in April?\\n    - How many clips did Natalia sell in May?\\nConfidence level: stop']\n",
    "    # print('responses:')\n",
    "    # print(responses)\n",
    "    candidates_to_scores = {}\n",
    "    for response in responses:\n",
    "        completion_tokens += len(response.split())\n",
    "        parsed_response = parse_response(response.lower())\n",
    "        # print(parsed_response)\n",
    "        if parsed_response:\n",
    "            for candidate, score in parsed_response:\n",
    "                # candidate = str(candidate)\n",
    "                candidates_to_scores[candidate] = candidates_to_scores.get(candidate, 0) + score # sums up the scores of same candidates \n",
    "        # choose candiate with highest score\n",
    "    # print(sorted(candidates_to_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    env.cache[obs] = candidates_to_scores # Stores the obtained candidates and scores in the cache of the environment for future use\n",
    "    return candidates_to_scores\n",
    "\n",
    "def propose_score(env, idx):\n",
    "    obs = env.reset(idx)\n",
    "    done = False\n",
    "    infos = []\n",
    "    while not done:\n",
    "        responses = gpt(propose_prompt_wrap(obs), n=5)\n",
    "        candidates_to_scores = {}\n",
    "        for response in responses:\n",
    "            parsed_response = parse_response(response)\n",
    "            if parsed_response:\n",
    "                for candidate, score in parsed_response:\n",
    "                    candidates_to_scores[candidate] = candidates_to_scores.get(candidate, 0) + score # aggregated scores for each candidate. If a candidate is already present in the dictionary, the existing score is incremented by the new score. If the candidate is not present, a new entry is created with the initial score.\n",
    "        # choose candiate with highest score\n",
    "        print(sorted(candidates_to_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        if len(candidates_to_scores) == 0:\n",
    "            break\n",
    "        candidates =  sorted(candidates_to_scores, key=candidates_to_scores.get, reverse=True)\n",
    "        for candidate in candidates:\n",
    "            env_ = copy.deepcopy(env)\n",
    "            env_.step(candidate)\n",
    "            if not any(_ == 2 for _ in env_.status):\n",
    "                break\n",
    "        print(candidate)\n",
    "        # candidate = input()\n",
    "        obs, r, done, info = env.step(candidate)\n",
    "        print(obs)\n",
    "        print(env.steps, info)\n",
    "        print('-------------------\\n\\n\\n')\n",
    "        infos.append(info)\n",
    "    return infos\n",
    "\n",
    "def is_leaf_node(list):\n",
    "    leaf_nodes = []\n",
    "    for i in range(len(list)):\n",
    "        line = list[i].strip()\n",
    "        if line.startswith('-'):\n",
    "            if i + 1 < len(list) and list[i + 1].strip().startswith('\\t-'):\n",
    "                # If followed by a question with tab and hyphen, add that instead\n",
    "                leaf_nodes.append(list[i + 1].strip())\n",
    "            else:\n",
    "                leaf_nodes.append(line)\n",
    "    return leaf_nodes\n",
    "\n",
    "def get_ans(env, vectordb, qa_chain):\n",
    "    question_list = env.decomp.split('\\n')\n",
    "\n",
    "    result = []\n",
    "    for sub_question in question_list:\n",
    "        if sub_question in is_leaf_node(question_list):\n",
    "            docs = vectordb.similarity_search(sub_question, k=3)\n",
    "            print(docs)\n",
    "            answer = qa_chain({\"query\": sub_question})\n",
    "            answer = answer[\"result\"]\n",
    "            result.append(f\"{sub_question} {answer}\")\n",
    "        else:\n",
    "            result.append(sub_question)\n",
    "    # print(result)\n",
    "    result_text = '\\n'.join(result)\n",
    "    # print(result_text)\n",
    "    return result_text\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def dfs(env, actions, infos, time_limit, prune, max_per_state, done=False):\n",
    "    global completion_tokens, prompt_tokens\n",
    "    # get candidate thoughts\n",
    "    candidates_to_scores = get_candidates_to_scores(env)\n",
    "    # if len(candidates_to_scores) == 0: return 0, [], []\n",
    "    if len(candidates_to_scores) == 0: \n",
    "        done = True\n",
    "        return env, done\n",
    "    print(\"sorted candidates to score:\")\n",
    "    print(sorted(candidates_to_scores.items(), key=lambda x: x[1], reverse=True)) # Prints the candidates and their scores sorted in descending order based on confidence levels\n",
    "\n",
    "    # back up current state\n",
    "    decomp, status, steps = copy.copy(env.decomp), env.status.copy(), env.steps\n",
    "\n",
    "    # try each candidate\n",
    "    cnt_per_state = 0\n",
    "    for action in sorted(candidates_to_scores, key=candidates_to_scores.get, reverse=True): # Iterates over candidates in descending order of their confidence levels\n",
    "        # obs, r, done, info = env.step(action) # Takes a step in the environment with the selected action\n",
    "        obs = env.step(action)\n",
    "        print('### action ###')\n",
    "        print(obs)\n",
    "        # r = info['r_word'] # collects reward for this action\n",
    "        if len(infos) < time_limit and env.steps < 2 and not any(_ == 2 for _ in env.status) and candidates_to_scores[action] < 100:  # not violating any existing constraints\n",
    "            cnt_per_state += 1\n",
    "            if cnt_per_state > max_per_state: break\n",
    "            count, prompt, res = env.prompt_status()   \n",
    "            prompt_tokens += len(prompt.split())\n",
    "            completion_tokens += len(res.split())      \n",
    "            actions.append(action)  \n",
    "            \n",
    "            info = {'total_step': len(infos), 'env_step': env.steps, 'actions': actions.copy(), 'count': count} # Information about the explored states is collected\n",
    "            infos.append(info)\n",
    "            print(info)\n",
    "\n",
    "            if not prune or count['impossible'] < 1:  # only continue if the current status is possible and is not pruned\n",
    "                print(\"DFS recursive call\")\n",
    "                env, done = dfs(env, actions, infos, time_limit, prune, max_per_state, done) ### RECURSIVELY CALLS DFS FOR FURTHER EXPLORATION ###\n",
    "            \n",
    "            if done: break\n",
    "            actions.pop() # Pops the last action from the list of actions to backtrack (undoing the last decision made during the exploration)\n",
    "            # print('remaining actions:')\n",
    "            # print(actions)\n",
    "            \n",
    "        if len(infos) >= time_limit or env.steps >= 4 or candidates_to_scores[action] >= 100:\n",
    "            info = {'total_step': len(infos), 'env_step': env.steps, 'decomp_state': env.decomp}\n",
    "            infos.append(info)\n",
    "            print(info)\n",
    "\n",
    "            if len(infos) >= time_limit:\n",
    "                reason = 'time out'\n",
    "            elif env.steps >= 4:\n",
    "                reason = 'environment steps'\n",
    "            else:\n",
    "                reason = 'decomposition finished'\n",
    "\n",
    "            print(f'$$$ end exploration because of: {reason} $$$')\n",
    "            done = True\n",
    "            break\n",
    "        else:\n",
    "            env.reset(env.idx, decomp=copy.copy(decomp), status=status.copy(), steps=steps) # if all nodes are impossible or the max amount of steps was reached, resets the environment to the previous layer's backed-up state to backtrack and explore other candidate actions\n",
    "            print('$$$ backtrack $$$')\n",
    "        # print(env.render())\n",
    "\n",
    "    # print(env.render())\n",
    "    return env, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs with pruning\n",
    "resultss = []\n",
    "for iter in range(3):\n",
    "        infoss = []\n",
    "        for i in range(0, 100, 5):\n",
    "                env.reset(i)\n",
    "                infos = []\n",
    "                actions = []\n",
    "\n",
    "                start_time = time.time()\n",
    "                final_env, _ = dfs(env, actions, infos, 6, prune=True, max_per_state=2)\n",
    "                score, prompt, res = env.eval_status()    \n",
    "                answer = get_ans(final_env, vectordb, qa_chain)\n",
    "                answer = gpt(answer_prompt_wrap(answer, final_env.data), temperature=0)\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time - start_time\n",
    "\n",
    "                prompt_tokens += len(prompt.split())\n",
    "                completion_tokens += len(res.split()) \n",
    "                completion_tokens += len(str(answer).split()) \n",
    "                print(\"and the answer is: \"+str(answer))\n",
    "                gpt_cost = gpt_usage()\n",
    "                new_entry = {\"answer\": answer, \"answer_gt\": env.ans_gt, \"gpt usage\": gpt_cost['cost'], \"elapsed time\": elapsed_time, \"scores\": score}\n",
    "                infos.append(new_entry)\n",
    "                infoss.append(infos)\n",
    "                with open('../../logs/strategy_qa/infoss_dfs_retrieval_gpt35.json', 'w') as fout:\n",
    "                # with open('../../logs/strategy_qa/test_retrieval.json', 'w') as fout:\n",
    "                        json.dump(infoss, fout, indent=4)\n",
    "        \n",
    "        with open('../../logs/strategy_qa/infoss_dfs_gpt35.json', 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "        results = compute_f1_strategy(data)\n",
    "        resultss.append(results)\n",
    "\n",
    "        with open('../../logs/strategy_qa/results_retrieval.json', 'w') as file:\n",
    "                json.dump(resultss, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../logs/strategy_qa/results_retrieval.json', 'r') as file:\n",
    "    data_lists = json.load(file)\n",
    "\n",
    "combined_data = []\n",
    "for data_list in data_lists:\n",
    "    combined_data.extend(data_list)\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for item in combined_data:\n",
    "    f1 = item['f1']\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f1_scores)\n",
    "average_f1_score = sum(f1_scores)/ len(f1_scores)\n",
    "print(f\"Average F1 score: {average_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
